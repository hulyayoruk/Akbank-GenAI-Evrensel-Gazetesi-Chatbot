import os
import streamlit as st
from dotenv import load_dotenv # .env dosyasn okumak i√ßin

# --- YENƒ∞: .env dosyasn y√ºkle ---
# Bu satr, projenin ba≈ündaki .env dosyasn arar ve i√ßindeki deƒüi≈ükenleri y√ºkler.
load_dotenv()

# LangChain k√ºt√ºphanelerini i√ße aktar
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings # 0.2.x+ s√ºr√ºm√º i√ßin doƒüru import
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# --- 1. Model ve Veritaban Y√ºkleme Fonksiyonlar ---

@st.cache_resource
def load_models_and_db():
    """
    Lokal embedding modelini ve FAISS veritabann y√ºkler.
    """
    print("Lokal embedding modeli y√ºkleniyor...")
    embeddings = HuggingFaceEmbeddings(
        model_name="all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'}
    )
    
    print("FAISS veritaban y√ºkleniyor...")
    db_path = "faiss_index"
    vector_db = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
    retriever = vector_db.as_retriever(search_kwargs={'k': 3})
    if not os.path.exists(db_path):
        st.error(f"HATA: 'faiss_index' klas√∂r√º bulunamad. L√ºtfen √∂nce 'process_data.py' betiƒüini √ßal≈ütrn.")
        return None, None
        
    
    
    print("Modeller ve veritaban y√ºklendi.")
    return retriever

@st.cache_resource
def load_llm(google_api_key):
    """
    Gemini LLM modelini y√ºkler.
    """
    print("Gemini LLM y√ºkleniyor...")
    try:
        llm = ChatGoogleGenerativeAI(
            model="gemini-flash-latest", # GitHub'da bulduƒüunuz doƒüru model ad
            google_api_key=google_api_key,
            temperature=0.3
        )
        return llm
    except Exception as e:
        st.error(f"Gemini modeli y√ºklenirken hata olu≈ütu: {e}")
        return None

# --- 2. RAG Pipeline (Zincir) Olu≈üturma ---
def create_rag_chain(retriever, llm):
    template = """
    Sana verilen baƒülam (context) kullanarak kullanc sorusunu cevapla.
    Baƒülam, Evrensel gazetesinden alnan haber metinleridir.
    Cevabn sadece sana verilen baƒülamdaki bilgilere dayanmaldr.
    Eƒüer baƒülamda cevap yoksa, 'Bu konuda bilgim yok.' de.
    
    Baƒülam (Context):
    {context}
    
    Soru (Question):
    {question}
    
    Cevap:
    """
    prompt = PromptTemplate(
        template=template,
        input_variables=["context", "question"]
    )
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return rag_chain

# --- 3. Streamlit Aray√ºz√º ---

st.set_page_config(page_title="Evrensel Haber Chatbot", layout="wide")
st.title("üì∞ Evrensel Gazetesi RAG Chatbot")
st.markdown("Evrensel gazetesinin 'Son 24 Saat' haberleri hakknda sorular sorun.")
st.markdown("---")

# --- YENƒ∞: API ANAHTARINI .env DOSYASINDAN OKUMA ---
# Aray√ºzden istemek yerine, .env dosyasndan y√ºklenen anahtar alyoruz.
google_api_key = os.getenv("GOOGLE_API_KEY")

if not google_api_key:
    st.error("HATA: Google API Anahtar bulunamad!")
    st.error("L√ºtfen proje ana klas√∂r√ºnde '.env' adnda bir dosya olu≈üturun ve i√ßine 'GOOGLE_API_KEY=\"AIza...\"' ≈üeklinde anahtarnz ekleyin.")
    st.stop() # Anahtar yoksa uygulamay durdur

# Ana bile≈üenleri y√ºkle
retriever = load_models_and_db()
llm = load_llm(google_api_key)

if retriever and llm:
    rag_chain = create_rag_chain(retriever, llm)
    if "messages" not in st.session_state:
        st.session_state.messages = []
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    if prompt := st.chat_input("Haberler hakknda bir soru sorun..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        with st.chat_message("assistant"):
            try:
                with st.spinner("Haberler aranyor ve cevap olu≈üturuluyor..."):
                    response = rag_chain.invoke(prompt)
                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
            except Exception as e:
                if "429" in str(e) and "quota" in str(e).lower():
                    st.error("HATA: Google API kotanz (Gemini) a≈ütnz. L√ºtfen daha sonra tekrar deneyin veya plannz kontrol edin.")
                else:
                    st.error(f"Cevap alnrken bir hata olu≈ütu: {e}")
else:
    st.error("Uygulama ba≈ülatlamad. L√ºtfen 'faiss_index' klas√∂r√ºn√ºn olduƒüundan ve API anahtarnzn doƒüru olduƒüundan emin olun.")