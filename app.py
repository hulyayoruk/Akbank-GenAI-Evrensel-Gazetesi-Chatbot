import os

import streamlit as st
from dotenv import load_dotenv

# .env dosyasÄ±ndaki API anahtarÄ±nÄ± yÃ¼kle
load_dotenv()

# LangChain kÃ¼tÃ¼phanelerini iÃ§e aktar
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# --- 1. Model ve VeritabanÄ± YÃ¼kleme FonksiyonlarÄ± ---

@st.cache_resource
def load_models_and_db():
    """
    Lokal embedding modelini ve FAISS veritabanÄ±nÄ± yÃ¼kler.
    """
    print("Lokal embedding modeli yÃ¼kleniyor...")
    embeddings = HuggingFaceEmbeddings(
        model_name="all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'}
    )
    
    print("FAISS veritabanÄ± yÃ¼kleniyor...")
    db_path = "faiss_index"
    if not os.path.exists(db_path):
        st.error(f"HATA: 'faiss_index' klasÃ¶rÃ¼ bulunamadÄ±. LÃ¼tfen Ã¶nce 'process_data.py' betiÄŸini yerel makinenizde Ã§alÄ±ÅŸtÄ±rÄ±n.")
        return None
        
    vector_db = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
    
    # --- RETRIEVER  ---
    # Arama sonuÃ§larÄ±nÄ±n kalitesini ve Ã§eÅŸitliliÄŸini artÄ±rmak iÃ§in MMR kullanÄ±lÄ±yor.
    retriever = vector_db.as_retriever(search_type="mmr", search_kwargs={'k': 10, 'fetch_k': 30})
    
    print("Modeller ve veritabanÄ± yÃ¼klendi.")
    return retriever

@st.cache_resource
def load_llm(google_api_key):
    """
    Gemini LLM modelini yÃ¼kler.
    """
    print("Gemini LLM yÃ¼kleniyor...")
    try:
        llm = ChatGoogleGenerativeAI(
            model="gemini-flash-latest",
            google_api_key=google_api_key,
            temperature=0.3
        )
        return llm
    except Exception as e:
        st.error(f"Gemini modeli yÃ¼klenirken hata oluÅŸtu: {e}")
        return None

# --- 2. RAG Pipeline (Zincir) OluÅŸturma  ---

def create_rag_chain(retriever, llm):
    """
    Verilen retriever ve llm ile RAG zincirini oluÅŸturur.
    """
    system_prompt = (
        "Sen Evrensel gazetesinin haberleri hakkÄ±nda bilgi veren bir asistansÄ±n. "
        "KullanÄ±cÄ± sorusunu anlamaya Ã§alÄ±ÅŸ ve sadece baÄŸlamda (context) geÃ§en haberleri temel alarak cevap ver. "
        "BaÄŸlamdaki bilgiyi anlam olarak da deÄŸerlendir, yani isim, olay veya kiÅŸi eÅŸleÅŸmelerini semantik olarak ara. "
        "CevabÄ±nÄ±n sonunda kullandÄ±ÄŸÄ±n haberlerin kaynak linklerini 'Kaynak:' baÅŸlÄ±ÄŸÄ± altÄ±nda listele. "
        "BaÄŸlamda hiÃ§bir bilgi yoksa 'Bu konuda bilgim yok.' de."
    )
    human_prompt = (
        "BaÄŸlam (Context):\n{context}\n\nSoru (Question):\n{question}"
    )
    prompt = ChatPromptTemplate.from_messages([("system", system_prompt), ("human", human_prompt)])
    
    def format_docs(docs):
        # Kaynak linklerini de ekleyerek daha zengin bir baÄŸlam oluÅŸturuyoruz.
        formatted_strings = []
        for doc in docs:
            source = doc.metadata.get('source', 'Kaynak BulunamadÄ±')
            formatted_strings.append(f"Kaynak: {source}\n{doc.page_content}")
        return "\n\n---\n\n".join(formatted_strings)

    # --- ZÄ°NCÄ°R YAPISI ---

    answer_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    # Cevap zincirini ve kaynaklarÄ± getiren zinciri birleÅŸtiren son yapÄ±
    rag_chain = RunnableParallel(
        answer=answer_chain,
        sources=retriever,
    )
    return rag_chain

# --- 3. Streamlit ArayÃ¼zÃ¼ ---

st.set_page_config(page_title="Evrensel Haber Chatbot", layout="wide")
st.title("ğŸ“° Evrensel Gazetesi RAG Chatbot")
st.markdown("Evrensel gazetesinin 'Son 24 Saat' haberleri hakkÄ±nda sorular sorun.")
st.markdown("---")



google_api_key = os.getenv("GOOGLE_API_KEY")
if not google_api_key:
    st.error("HATA: Google API AnahtarÄ± bulunamadÄ±!")
    st.info("Bu hatayÄ± yerel makinenizde alÄ±yorsanÄ±z, lÃ¼tfen proje ana klasÃ¶rÃ¼nde `.env` adÄ±nda bir dosya oluÅŸturun. EÄŸer bu hata deploy edilmiÅŸ sitede gÃ¶rÃ¼nÃ¼yorsa, Streamlit Cloud'un 'Secrets' bÃ¶lÃ¼mÃ¼ne API anahtarÄ± eklenmelidir.")
    st.stop()

retriever = load_models_and_db()
llm = load_llm(google_api_key)

if retriever and llm:
    rag_chain = create_rag_chain(retriever, llm)
    if "messages" not in st.session_state:
        st.session_state.messages = []
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            
    if user_prompt := st.chat_input("Haberler hakkÄ±nda bir soru sorun..."):
        st.session_state.messages.append({"role": "user", "content": user_prompt})
        with st.chat_message("user"):
            st.markdown(user_prompt)
            
        with st.chat_message("assistant"):
            try:
                with st.spinner("Haberler aranÄ±yor ve cevap oluÅŸturuluyor..."):
                    # Zincir artÄ±k doÄŸrudan cevabÄ± (string) dÃ¶ndÃ¼rÃ¼r.
                    response = rag_chain.invoke(user_prompt)
                st.markdown(response)

        # CevabÄ± ve debug bilgisini ekrana yazdÄ±r
                answer = response['answer']
                sources = response['sources']
                st.markdown(answer)
                
                st.session_state.messages.append({"role": "assistant", "content": response})
            except Exception as e:
                st.error(f"Cevap alÄ±nÄ±rken bir hata oluÅŸtu: {e}")
else:
    st.error("Uygulama baÅŸlatÄ±lamadÄ±. LÃ¼tfen 'faiss_index' klasÃ¶rÃ¼nÃ¼n olduÄŸundan ve API anahtarÄ±nÄ±zÄ±n doÄŸru olduÄŸundan emin olun.")

